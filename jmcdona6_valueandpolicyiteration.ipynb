{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework on Value and Policy Iteration",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliammcdonald/6884psets/blob/main/jmcdona6_valueandpolicyiteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrvVF3c0_9WM"
      },
      "source": [
        "# Spring 2021 6.884 Computational Sensorimotor Learning Assignment 2\n",
        "\n",
        "In this assignment, we will implement three principle reinforcement learning algorithms which provably converge to optimal solutions for MDPs:\n",
        "\n",
        "\n",
        "*   Value iteration\n",
        "*   Policy iteration\n",
        "*   Q-learning\n",
        "\n",
        "You will need to <font color='blue'>answer the bolded questions</font> and <font color='blue'>fill in the missing code snippets (marked by **TODO**)</font>.\n",
        "\n",
        "There are (approximately) **239** total points to be had in this PSET.  `ctrl-f` for \"pts\" to ensure you don't miss questions.\n",
        "\n",
        "**_Please fill in your name below:_**\n",
        "\n",
        "**Name**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIwE_4jt9JgR"
      },
      "source": [
        "### Credits\n",
        "\n",
        "Some part of the code of this assignment is borrowed from the Spring 2018 CMU Deep Reinforcement Learning & Control course. We also thank Prof. [Cathy Wu](https://idss.mit.edu/staff/cathy-wu/) for polishing the content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilgy6DEx_z65"
      },
      "source": [
        "## Setup\n",
        "\n",
        "The following code sets up imports and helper functions (you can ignore this)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx58PD3jc40l"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "import gym\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from copy import deepcopy\n",
        "from tqdm.notebook import tqdm\n",
        "from dataclasses import dataclass\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "from typing import Any\n",
        "from collections import deque\n",
        "\n",
        "mpl.rcParams['figure.dpi']= 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgD4VlhkPeYS"
      },
      "source": [
        "# some util functions\n",
        "def plot(logs, x_key, y_key, legend_key, **kwargs):\n",
        "    nums = len(logs[legend_key].unique())\n",
        "    palette = sns.color_palette(\"hls\", nums)\n",
        "    if 'palette' not in kwargs:\n",
        "        kwargs['palette'] = palette\n",
        "    sns.lineplot(x=x_key, y=y_key, data=logs, hue=legend_key, **kwargs)\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "# set random seed\n",
        "seed = 0\n",
        "set_random_seed(seed=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi-_JUbdTad7"
      },
      "source": [
        "# FrozenLake environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPZpix6-C8XN"
      },
      "source": [
        "*Winter has come.* You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
        "\n",
        "The surface is described using a grid like the following:\n",
        "\n",
        "```\n",
        "SFFF # (S: starting point, safe)\n",
        "FHFH # (F: frozen surface, safe)\n",
        "FFFH # (H: hole, fall to your doom)\n",
        "HFFG # (G: goal, where the frisbee is located)\n",
        "```\n",
        "\n",
        "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
        "\n",
        "Here's what the Frozen Lake looks like in action, when following a random agent:\n",
        "\n",
        "![](https://miro.medium.com/max/690/1*ur_42c7MLhbi6q2L3JtSqg.gif)\n",
        "\n",
        "Frozen Lake is part of OpenAI gym, a collection of open-source environments for benchmarking RL algorithms.   [Here](https://gym.openai.com/envs/FrozenLake-v0/) is a link to the gym environment (also the source of our environment description)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrX2DmlLsAkM"
      },
      "source": [
        "**Question:** how many actions can the agent take at any time; eg, what is the size of the action space? (2 pts)\n",
        "\n",
        "**Answer:** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PprRCSgFsrZI"
      },
      "source": [
        "Now, here's some code that creates the above environment through OpenAI gym, called `FrozenLake-v0`.  Note that we will be using a deterministic variant, so providing an action in a given direction will always move you in that direction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23UivOJriyde"
      },
      "source": [
        "## create FrozenLake environment\n",
        "MAPS = {\n",
        "    \"4x4\": [\n",
        "        \"GHFS\",\n",
        "        \"FHHF\",\n",
        "        \"FFHF\",\n",
        "        \"FFFF\"\n",
        "    ],\n",
        "    \"8x8\": [\n",
        "        \"FFFSFFFF\",\n",
        "        \"FFFFFFFF\",\n",
        "        \"HHHHFHFF\",\n",
        "        \"FFFFFFHF\",\n",
        "        \"FFFFFFFF\",\n",
        "        \"FHFFFFHF\",\n",
        "        \"FHFFHFHH\",\n",
        "        \"FGHFFFFF\"\n",
        "    ],\n",
        "}\n",
        "from gym.envs.registration import register\n",
        "env_name = 'Deterministic-4x4-FrozenLake-v0'\n",
        "if env_name not in gym.envs.registry.env_specs:\n",
        "    register(\n",
        "        id=env_name,\n",
        "        entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "        kwargs={'map_name': None,\n",
        "                'is_slippery': False,\n",
        "                'desc': MAPS['4x4']\n",
        "                },\n",
        "        max_episode_steps=20)\n",
        "\n",
        "env_name = 'Deterministic-8x8-FrozenLake-v0'\n",
        "if env_name not in gym.envs.registry.env_specs:\n",
        "    register(\n",
        "        id=env_name,\n",
        "        entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "        kwargs={'map_name': None,\n",
        "                'is_slippery': False,\n",
        "                'desc': MAPS['8x8'],\n",
        "                },\n",
        "        max_episode_steps=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCBb9JESEMqN"
      },
      "source": [
        "We would like to find a good policy for the agent (you, the brave soul). More precisely, the agent controls the movement of a character on the frozen lake. The frozen lake environment is an example of a _grid world_, since it consists of objects moving around in a discrete (grid) world. Grid world problems can constitute or approximate a wide class of problems.\n",
        "\n",
        "**Question:** Consider a racecar environment, where the goal is to get the agent (the racecar) around a race track as quickly as possible. Is this suitable for representing as a grid world problem?  Justify your answer. (4 pts)\n",
        "\n",
        "**Answer:** \n",
        "\n",
        "Now let's return to the task at hand: retrieving our frisbee!\n",
        "Remember that some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
        "\n",
        "Now let's consider some algorithms for solving this problem, i.e. finding a good policy to accomplish the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdJjohytByCv"
      },
      "source": [
        "# Value Iteration\n",
        "\n",
        "Value iteration is the first algorithm we will consider.\n",
        "\n",
        "Let's first do a sanity check. In class, we learned that value iteration is model-based and as such, it is best for problems with small state spaces.\n",
        "\n",
        "**Question:** Consider a fixed 4x4-grid FrozenLake. What is the size of the state space? (4 pts)\n",
        "\n",
        "**Answer:** \n",
        "\n",
        "Now recall from class that value iteration is a model-based method which starts with any guess of a value function $V_0$ and then updates it according to the optimal Bellman equation: $V_{i+1}(s) = \\mathcal{T} V_i(s) = \\max_a \\mathbb{E}_{s' \\sim p(\\cdot | s, a)}[r(s, a, s') + \\gamma V_i(s')]$.  In our case, as we have a fixed number of states, our value function is simply a mapping of square -> value (eg an array).\n",
        "\n",
        "*Note:* we choose to represent actions as integers with the following mapping:\n",
        "\n",
        "```\n",
        "LEFT = 0\n",
        "DOWN = 1\n",
        "RIGHT = 2\n",
        "UP = 3\n",
        "```\n",
        "\n",
        "and states as zero-indexed integers traversing from the top left."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGcN5-56TdxS"
      },
      "source": [
        "## Implement Value Iteration\n",
        "\n",
        "*(20 pts)*\n",
        "\n",
        "We will now implement value iteration over the MDP with transition probabilities described by `env.P` (transition probabilites), `env.nS` (number of states), and `env.nA` (number of actions). The entry `env.P[s][a]` (where `s` is the state index and `a` is the action index) is a list of transition tuples $(p(s,a,s'), s', r(s, a, s'), \\text{episode_end})$ for each list index index `s'`.  In plain english:\n",
        "\n",
        " - $p(s,a,s')$: the probability of transitioning to state `s'` after taking action `a` in state `s`.\n",
        " - $s'$: the next state under this transition.\n",
        " - $r(s, a, s')$: the reward of taking this transition.\n",
        " - `episode_end`: a boolean representing whether taking this action ends the episode (in Frozen lLke, whether this kills you or takes you to the goal position)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OknrYASeB9fC"
      },
      "source": [
        "def value_iteration(env, gamma, max_iterations=1000, tol=1e-3):\n",
        "    \"\"\"Runs value iteration for a given gamma and environment.\n",
        "    Updates states in their 1-N order.\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "      The environment to compute value iteration for. Must have nS,\n",
        "      nA, and P as attributes.\n",
        "    gamma: float\n",
        "      Discount factor, must be in range [0, 1)\n",
        "    max_iterations: int\n",
        "      The maximum number of iterations to run before stopping.\n",
        "    tol: float\n",
        "      Determines when value function has converged.\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray, iteration, list\n",
        "      The value function, the number of iterations it took to converge, and a list\n",
        "      of the value functions after each iteration.\n",
        "    \"\"\"\n",
        "    value_func = np.zeros(env.nS) # initial value function: all states are zero\n",
        "    policy = np.zeros(env.nS, dtype='int') # placeholder for computed policy\n",
        "    iters = 0\n",
        "    value_history = []\n",
        "    while True:\n",
        "        delta = 0\n",
        "        ###### TODO: value function update ##############\n",
        "\n",
        "        ################################################################\n",
        "        \n",
        "        # let's also save a copy of value function after each iteration\n",
        "        value_history.append(value_func.copy())\n",
        "        iters += 1\n",
        "        if delta < tol or iters >= max_iterations:\n",
        "            break\n",
        "    return value_func, iters, value_history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se-exLs_mNrQ"
      },
      "source": [
        "Actually, computing the optimal value function is not enough. What we are interested in is the optimal policy, not just how good each state is. Luckily, the optimal value function and the optimal policy are related. Let's implement this next:\n",
        "\n",
        "*(15 pts)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMV--rNqmMpJ"
      },
      "source": [
        "def value_function_to_policy(env, gamma, value_function):\n",
        "    \"\"\"Output action numbers for each state in value_function.\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "      Environment to compute policy for. Must have nS, nA, and P as\n",
        "      attributes.\n",
        "    gamma: float\n",
        "      Discount factor. Number in range [0, 1)\n",
        "    value_function: np.ndarray\n",
        "      Value of each state.\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray\n",
        "      An array of integers. Each integer is the optimal action to take\n",
        "      in that state according to the environment dynamics and the\n",
        "      given value function.\n",
        "    \"\"\"\n",
        "    policy = np.zeros(env.nS, dtype='int')\n",
        "    for idx in range(env.nS):\n",
        "        p = env.P[idx]\n",
        "        vmax = -np.inf\n",
        "        best_act = -1\n",
        "        ###### TODO: Select the best action (best_act) ##############\n",
        "        \n",
        "        ###################################################\n",
        "\n",
        "        policy[idx] = best_act\n",
        "    return policy\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGVvR2gwmuwg"
      },
      "source": [
        "**Question**: What is the difference between the value iteration algorithm and extracting a policy from a value function? (4 pts)\n",
        "\n",
        "**Answer**: \n",
        "\n",
        "OK enough talk, let's run it. We'll consider this 8x8 frozen lake:\n",
        "\n",
        "```\n",
        "FFFSFFFF\n",
        "FFFFFFFF\n",
        "HHHHFHFF\n",
        "FFFFFFHF\n",
        "FFFFFFFF\n",
        "FHFFFFHF\n",
        "FHFFHFHH\n",
        "FGHFFFFF\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ros3SnX6Th04"
      },
      "source": [
        "## Run Value Iteration\n",
        "\n",
        "*(10 pts)*\n",
        "\n",
        "Print and plot your computed value and policy functions as 8x8 2D grids.  Your policy should look something like this:\n",
        "\n",
        "```\n",
        "XXXXXXXX\n",
        "XXXXXXXX\n",
        "...\n",
        "XXXXXXXX\n",
        "```\n",
        "\n",
        "where `X` is the optimal action in that state (one of `L`, `R`, `U`, `D`).  Your value function should look similar, except containing space-separated floats rather than action characters.\n",
        "\n",
        "For `plot_value_history`, plot each value function in the history using the `plt.imgshow` method from `matplotlib`.  To plot multiple images at once, you may find `plt.subplot` useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmMVbUDb0Bs7"
      },
      "source": [
        "def print_policy(policy):\n",
        "    ### TODO: implement method ################################\n",
        "    return\n",
        "    ##########################################################\n",
        "\n",
        "\n",
        "def print_value(value):\n",
        "    ### TODO: implement method ################################\n",
        "    return\n",
        "    ##########################################################\n",
        "\n",
        "def plot_value_history(value_history):\n",
        "    ### TODO: implement method ################################\n",
        "    return\n",
        "    ##########################################################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR2agkksCFOr"
      },
      "source": [
        "env = gym.make('Deterministic-8x8-FrozenLake-v0')\n",
        "gamma = 0.9\n",
        "value_info = value_iteration(env, gamma, max_iterations=int(1e3), tol=1e-4)\n",
        "value, iters, value_history = value_info\n",
        "policy = value_function_to_policy(env, gamma, value)\n",
        "print('Policy: ')\n",
        "print_policy(policy)\n",
        "print('Value: ')\n",
        "print_value(value)\n",
        "print('Iterations: ', iters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42PVPUZB1dht"
      },
      "source": [
        "plot_value_history(value_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2GBerlZQSJa"
      },
      "source": [
        "**Question**: Answers/observe the following *(18 pts)*:\n",
        "\n",
        "- Has the agent found a successful policy? (hint: it should have.)  How do you know? \n",
        "  - **Answer**: \n",
        "\n",
        "- Does the value function seem reasonable? Please explain. \n",
        "  - **Answer**: \n",
        "\n",
        "- How many steps of value iteration were required convergence? \n",
        "  - **Answer**: \n",
        "\n",
        "- How many steps does the agent take to reach the goal location?\n",
        "  - **Answer**: \n",
        "\n",
        "\n",
        "- Try running the above code with gamma = 0.3.  Does the agent converge to a successful policy (yes/no)? \n",
        "  - **Answer**: \n",
        "\n",
        "- If yes, then why does gamma have no effect? If no, then why does gamma matter? \n",
        "  - **Answer**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9_vtY44_uRz"
      },
      "source": [
        "# Policy Iteration\n",
        "\n",
        "Recall from class that value iteration is a special instance of generalized policy iteration, which alternates between 1 step of policy evaluation and 1 step of policy improvement.\n",
        "\n",
        "Now we'll consider policy iteration, which alternates between many steps of policy evaluation and 1 step of policy improvement. Does that feel silly?\n",
        "\n",
        "We will make a few notes:\n",
        "- Let's consider N steps of policy evaluation + 1 step of policy improvement to be 1 iteration of value/policy iteration.\n",
        "- It is not well understood why, but policy iteration and value iteration will attain the optimal policy in fewer iterations for different problems.\n",
        "- While policy iteration has a hidden cost of N policy evaluation steps, it turns out that a full policy evaluation can be computed efficiently, since it is a linear operation. (We will not do this in this assignment, but trust us.) If employed efficiently, policy iteration can be viewed as a super-powered value iteration, with accurate policy evaluation and without a whole lot of extra computational cost.\n",
        "\n",
        "Now, let's implement policy evaluation and policy improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PYcHg3f_11N"
      },
      "source": [
        "## Implement Policy Iteration\n",
        "\n",
        "*(15 pts)*\n",
        "\n",
        "First, implement the the high level wrapper `policy_iteration`, which evaluates a policy to obtain a `value_func` and feeds that into `improve_policy` to update the policy in a loop.  Remember to end iteration once the policy is stable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxF5TjsN9AR9"
      },
      "source": [
        "def policy_iteration(env, gamma, max_iterations=int(1e3), tol=1e-3):\n",
        "    \"\"\"Runs policy iteration using the improve_policy and evaluate_policy methods.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "      The environment to compute value iteration for. Must have nS,\n",
        "      nA, and P as attributes.\n",
        "    gamma: float\n",
        "      Discount factor, must be in range [0, 1)\n",
        "    max_iterations: int\n",
        "      The maximum number of iterations to run before stopping.\n",
        "    tol: float\n",
        "      Determines when value function has converged.\n",
        "    Returns\n",
        "    -------\n",
        "    (np.ndarray, np.ndarray, int, int, list)\n",
        "       Returns optimal policy, value function, number of policy\n",
        "       improvement iterations, number of value iterations, and a list\n",
        "       of the history value functions.\n",
        "    \"\"\"\n",
        "    policy = np.zeros(env.nS, dtype='int')\n",
        "    value_history = [] # should contain the value func after each policy iteration.\n",
        "    policy_imp_step = 0 # number of total policy iterations\n",
        "    policy_eval_step = 0 # number of total value iterations\n",
        "    while True:\n",
        "        ### TODO: Fill in policy iteration main loop. ##########\n",
        "        continue\n",
        "        ##########################################################\n",
        "    return policy, value_func, policy_imp_step, policy_eval_step, value_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dSV7riP4gta"
      },
      "source": [
        "Next, implement the policy evaluation and update submethods.\n",
        "\n",
        "*(25 pts)*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4loS9sw4g_-"
      },
      "source": [
        "def evaluate_policy(env, gamma, policy, max_iterations=int(1e3), tol=1e-3):\n",
        "    \"\"\"Performs policy evaluation.\n",
        "    Evaluates the value of a given policy by asynchronous DP.  Updates states in\n",
        "    their 1-N order.\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "      The environment to compute value iteration for. Must have nS,\n",
        "      nA, and P as attributes.\n",
        "    gamma: float\n",
        "      Discount factor, must be in range [0, 1)\n",
        "    policy: np.array\n",
        "      The policy to evaluate. Maps states to actions.\n",
        "    max_iterations: int\n",
        "      The maximum number of iterations to run before stopping.\n",
        "    tol: float\n",
        "      Determines when value function has converged.\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray, int\n",
        "      The value for the given policy and the number of iterations till\n",
        "      the value function converged.\n",
        "    \"\"\"\n",
        "    value_func = np.zeros(env.nS)\n",
        "    iter = 0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        ###### TODO: value function update (value_func) ##############\n",
        "\n",
        "        ###################################################\n",
        "\n",
        "        iter += 1\n",
        "        if delta < tol or iter >= max_iterations:\n",
        "            break\n",
        "    return value_func, iter\n",
        "\n",
        "def improve_policy(env, gamma, value_func, policy):\n",
        "    \"\"\"Performs policy improvement.\n",
        "    Given a policy and value function, improves the policy.\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "      The environment to compute value iteration for. Must have nS,\n",
        "      nA, and P as attributes.\n",
        "    gamma: float\n",
        "      Discount factor, must be in range [0, 1)\n",
        "    value_func: np.ndarray\n",
        "      Value function for the given policy.\n",
        "    policy: dict or np.array\n",
        "      The policy to improve. Maps states to actions.\n",
        "    Returns\n",
        "    -------\n",
        "    bool, np.ndarray\n",
        "      Returns true if policy changed. Also returns the new policy.\n",
        "    \"\"\"\n",
        "    policy_stable = True\n",
        "    new_policy = np.zeros(env.nS, dtype='int')\n",
        "    for idx in range(env.nS):\n",
        "        old_action = policy[idx]\n",
        "        p = env.P[idx]\n",
        "        new_action = -1\n",
        "        best_q = -np.inf\n",
        "        ###### TODO: use value function to get new action (new_action) ######\n",
        "        ###################################################\n",
        "        \n",
        "        new_policy[idx] = new_action\n",
        "        if new_action != old_action:\n",
        "            policy_stable = False\n",
        "    return policy_stable, new_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUXznsss_9eM"
      },
      "source": [
        "## Run Policy Iteration\n",
        "\n",
        "Assuming your above implementation is correct, you should be able to run the below code to evaluate policy iteration on Frozen Lake."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7zBqnPNUEaC"
      },
      "source": [
        "env = gym.make('Deterministic-8x8-FrozenLake-v0')\n",
        "gamma = 0.9\n",
        "policy_info = policy_iteration(env, gamma, max_iterations=int(1e3), tol=1e-3)\n",
        "new_policy, value_func, policy_imp_step, policy_eval_step, value_history = policy_info\n",
        "policy = value_function_to_policy(env, gamma, value)\n",
        "print('New policy: ')\n",
        "print_policy(new_policy)\n",
        "print('Value: ')\n",
        "print_value(value_func)\n",
        "print('Number of policy improvement steps: ', policy_imp_step)\n",
        "print('Total number of policy evaluation steps: ', policy_eval_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO7_e9-VUZKK"
      },
      "source": [
        "plot_value_history(value_history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk2seQKESHNv"
      },
      "source": [
        "Note that both value and policy iteration can solve the `Frozen Lake` environment (if one can't, you've done something wrong). \n",
        "\n",
        "**Questions** *(12 pts)*:\n",
        "- How many iterations (i.e. policy improvement steps) were required by policy iteration? \n",
        "  - **Answer:** \n",
        "- How many policy improvement steps were required by value iteration? \n",
        "  - **Answer:** \n",
        "- If one method took longer to converge, postulate an explanation for this.\n",
        "   - **Answer**: \n",
        "\n",
        "\n",
        "One common benchmark for reinforcment learning algorithms is *sample complexity*: the number of interactions the agent must have with the environment to learn a policy.  In policy iteration, we can approximate this as the number of \"actions\" for which policy improvement is run.\n",
        "\n",
        "**Question**:  Compute the sample complexity to solve 8x8 FrozenLake.  What number do you get? (10 pts)\n",
        "\n",
        "**Answer**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p2a-jhc8MuS"
      },
      "source": [
        "# Q-learning\n",
        "\n",
        "In the above two algorithms, we had access to the entire MDP: all of the states, with all of the transition probabilities between them.  Unfortunately, this is usually not the case.\n",
        "\n",
        "Below we will implement Q-learning, which is *model free*: it does not require full knowledge of environment dynamics, and instead will try to learn a policy purely through exploration and exploitation.\n",
        "\n",
        "Fill in the missing functions in the `QLearningAgent` below.\n",
        "\n",
        "*(50 pts)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXjEtjih8MWS"
      },
      "source": [
        "@dataclass\n",
        "class QLearningAgent:\n",
        "    env: gym.Env\n",
        "    learning_rate: float\n",
        "    gamma: float\n",
        "    initial_epsilon: float\n",
        "    min_epsilon: float\n",
        "    max_decay_episodes: int\n",
        "    init_q_value: float = 0.\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.num_states = env.nS\n",
        "        self.reset()\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        ### TODO: decay epsilon, called after every episode. ################\n",
        "        return\n",
        "        #####################################################################\n",
        "    \n",
        "    def reset(self):\n",
        "        self.epsilon = self.initial_epsilon\n",
        "        self.ep_reduction = (self.epsilon - self.min_epsilon) / float(self.max_decay_episodes)\n",
        "        self.Q = np.ones((self.num_states, self.env.nA)) * self.init_q_value\n",
        "\n",
        "    def update_Q(self, state, next_state, action, reward, done):\n",
        "        ### TODO: update self.Q given new experience. #######################\n",
        "        return\n",
        "        #####################################################################\n",
        "\n",
        "    def get_action(self, state):\n",
        "        ### TODO: select an action given self.Q and self.epsilon ############\n",
        "        return -1\n",
        "        #####################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upNSviFC99uV"
      },
      "source": [
        "The below code is scaffolding to instantiate and run the above Q-Learning agent.  Feel free to examine it to help you implement `QLearningAgent`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxxQPNH99zVh"
      },
      "source": [
        "@dataclass\n",
        "class QLearningEngine:\n",
        "    env: gym.Env\n",
        "    agent: Any\n",
        "    max_episodes: int\n",
        "    \n",
        "    def run(self, n_runs=1):\n",
        "        rewards = []\n",
        "        log = []\n",
        "        for i in tqdm(range(n_runs), desc='Runs'):\n",
        "            ep_rewards = []\n",
        "            self.agent.reset()\n",
        "            # we plot the smoothed return values\n",
        "            smooth_ep_return = deque(maxlen=100)\n",
        "            for t in tqdm(range(self.max_episodes), desc='Episode'):\n",
        "                state = self.env.reset()\n",
        "                ret = 0\n",
        "                while True:\n",
        "                    action = self.agent.get_action(state)\n",
        "                    next_state, reward, done, info = self.env.step(action)\n",
        "                    true_done = done and not info.get('TimeLimit.truncated', False)\n",
        "                    self.agent.update_Q(state, next_state, action, reward, true_done)\n",
        "                    ret += reward\n",
        "                    state = next_state\n",
        "                    if done:\n",
        "                        break\n",
        "                self.agent.decay_epsilon()\n",
        "                smooth_ep_return.append(ret)\n",
        "                ep_rewards.append(np.mean(smooth_ep_return))\n",
        "            rewards.append(ep_rewards)\n",
        "            run_log = pd.DataFrame({'return': ep_rewards,  \n",
        "                                    'episode': np.arange(len(ep_rewards)), \n",
        "                                    'iqv': self.agent.init_q_value})\n",
        "            log.append(run_log)\n",
        "        return log\n",
        "\n",
        "def qlearning_sweep(init_q_values, n_runs=4, max_episodes=60000, epsilon=0.8, learning_rate=0.8):\n",
        "    logs = dict()\n",
        "    pbar = tqdm(init_q_values)\n",
        "    agents = []\n",
        "    for iqv in pbar:\n",
        "        pbar.set_description(f'Initial q value:{iqv}')\n",
        "        env=gym.make('Deterministic-8x8-FrozenLake-v0')\n",
        "        agent = QLearningAgent(env=env,\n",
        "                               learning_rate=learning_rate,\n",
        "                               gamma=0.99,\n",
        "                               initial_epsilon=epsilon,\n",
        "                               min_epsilon=0.0,\n",
        "                               max_decay_episodes=max_episodes,\n",
        "                               init_q_value=iqv)\n",
        "        engine = QLearningEngine(env=env, agent=agent, max_episodes=max_episodes)\n",
        "        ep_log = engine.run(n_runs)\n",
        "        ep_log = pd.concat(ep_log, ignore_index=True)\n",
        "        logs[f'{iqv}'] = ep_log\n",
        "\n",
        "        agents.append(agent)\n",
        "    logs = pd.concat(logs, ignore_index=True)\n",
        "    return logs, agents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9jz-syH-Kze"
      },
      "source": [
        "Once the agent is implemented, run the below code to try it out on FrozenLake!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54EAQ5Fx8Qas"
      },
      "source": [
        "init_q_values = [0., 1.] # if it's 0, there is a chance that it can solve the problem.\n",
        "logs, agents = qlearning_sweep(init_q_values, n_runs=3, max_episodes=60000, epsilon=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDzZrEZf8RD3"
      },
      "source": [
        "plot(logs, x_key='episode', y_key='return', legend_key='iqv', estimator='mean', ci='sd')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM_yPqh98UgW"
      },
      "source": [
        "### Questions\n",
        "\n",
        "Using the above parameters (`init_q_values = [0., 1.]`, `epsilon=0.8`):\n",
        "- Print the policy for agents with each of the `init_q_values`. (6 pts)\n",
        "- For each initial Q value, do you converge to a successful policy (yes/no)? (4 pts)\n",
        "  - **Answer:**  \n",
        "- How does the performance compare between the initial Q values? Why is there a difference if any? (5 pts)\n",
        "  - **Answer:**  \n",
        "\n",
        "If you set `epsilon=0.0` and `init_q_value=1`:\n",
        "- How many steps does the policy take now? Why is it converging faster / slower as compared to higher value of initial_epsilon? (5 pts)\n",
        "  - **Answer:** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhGJnINI_M0J"
      },
      "source": [
        "### TODO: add policy for agents with each of the init q_values. ########\n",
        "########################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tiYCIqMA9Fi"
      },
      "source": [
        "## Q-Learning Sample Complexity\n",
        "\n",
        "Remember that we computed the *sample complexity* of Policy Iteration on 8x8 FrozenLake.\n",
        "\n",
        "Modify the `QLearningEngine` to compute the sample complexity under the same definition (the number of actions the agent must take to learn an optimal policy).  For our purposes, we'll define an optimal policy as when the average episode reward is around 1.\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "- What is the (rough) sample complexity of Q-Learning on 8x8 FrozenLake? (10 pts)\n",
        "  - **Answer**: \n",
        "- Compare the computed sample complexity of Q-Learning to Policy Iteration.  Discuss why these numbers may be so different, or so similar (20 pts).\n",
        "  - **Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff_YuBHOwx5x"
      },
      "source": [
        "# Optional, bonus points\n",
        "\n",
        "Please enter the bonus code you get after filling out the survey of this assignment.  The link to the survey is pinned on Piazza. (10 pts)\n",
        "\n",
        "**Bonus code**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpcdY5gH6IT6"
      },
      "source": [
        "End of PSET!"
      ]
    }
  ]
}